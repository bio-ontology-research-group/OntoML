{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# EL Embeddings\n\nThis example corresponds to the paper [EL Embeddings: Geometric Construction of Models for the Description Logic EL++](https://www.ijcai.org/proceedings/2019/845).\n\nThe idea of this paper is to embed EL by modeling ontology classes as $n$-dimensional balls ($n$-balls) and ontology object properties as transformations of those $n$-balls. For each of the normal forms, there is a distance function defined that will work as loss functions in the optimization framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's just define the imports that will be needed along the example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mowl\nmowl.init_jvm(\"10g\")\nfrom mowl.base_models.elmodel import EmbeddingELModel\nfrom mowl.models.elembeddings.evaluate import ELEmbeddingsPPIEvaluator\nfrom mowl.nn.elmodule import ELModule\nimport numpy as np\nimport torch as th\nfrom torch import nn\nfrom tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The EL-Embeddings model, maps ontology classes, object properties and operators into a\ngeometric model. The $\\mathcal{EL}$ description logic is expressed using the\nfollowing General Concept Inclusions (GCIs):\n\n\\begin{align}\\begin{align}\n   C &\\sqsubseteq D & (\\text{GCI 0}) \\\\\n   C_1 \\sqcap C_2 &\\sqsubseteq D & (\\text{GCI 1}) \\\\\n   C &\\sqsubseteq \\exists R. D & (\\text{GCI 2})\\\\\n   \\exists R. C &\\sqsubseteq D & (\\text{GCI 3})\\\\\n   C &\\sqsubseteq \\bot & (\\text{GCI BOT 0}) \\\\\n   C_1 \\sqcap C_2 &\\sqsubseteq \\bot & (\\text{GCI BOT 1}) \\\\\n   \\exists R. C &\\sqsubseteq \\bot & (\\text{GCI BOT 3})\n   \\end{align}\\end{align}\n\nwhere $C,C_1, C_2,D$ are ontology classes and $R$ is an ontology object property\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EL-Embeddings uses GCI 0, 1, 2, 3 and GCI BOT 1 (to express disjointness between classes).\nIn the use case of this example, we will test over a biological problem, which is\nprotein-protein interactions. Given two proteins $p_1,p_2$, the phenomenon\n\"$p_1$ interacts with $p_2$\" is encoded using GCI 2 as:\n\n\\begin{align}p_1 \\sqsubseteq interacts\\_with. p_2\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of the model and the loss functions\n\nIn this part we define the neural network part. As mentioned earlier, ontology classes \\\nare $n$-dimensional balls. Each ball has a center $c \\in \\mathbb{R}^n$ and \\\nradius $r \\in \\mathbb{R}$. $n$ will be the embedding size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ELEmModule(ELModule):\n\n    def __init__(self, nb_ont_classes, nb_rels, embed_dim=50, margin=0.1):\n        super().__init__()\n        self.nb_ont_classes = nb_ont_classes\n        self.nb_rels = nb_rels\n\n        self.embed_dim = embed_dim\n\n        # Embedding layer for classes centers.\n        self.class_embed = nn.Embedding(self.nb_ont_classes, embed_dim)\n        nn.init.uniform_(self.class_embed.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.class_embed.weight.data, axis=1).reshape(-1, 1)\n        self.class_embed.weight.data /= weight_data\n\n        # Embedding layer for classes radii.\n        self.class_rad = nn.Embedding(self.nb_ont_classes, 1)\n        nn.init.uniform_(self.class_rad.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.class_rad.weight.data, axis=1).reshape(-1, 1)\n        self.class_rad.weight.data /= weight_data\n\n        # Embedding layer for ontology object properties\n        self.rel_embed = nn.Embedding(nb_rels, embed_dim)\n        nn.init.uniform_(self.rel_embed.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.rel_embed.weight.data, axis=1).reshape(-1, 1)\n        self.rel_embed.weight.data /= weight_data\n\n        self.margin = margin\n\n    # Regularization method to force n-ball to be inside unit ball\n    def class_reg(self, x):\n        res = th.abs(th.linalg.norm(x, axis=1) - 1)\n        res = th.reshape(res, [-1, 1])\n        return res\n\n    # Loss function for normal form :math:`C \\sqsubseteq D`\n    def gci0_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n        rc = th.abs(self.class_rad(data[:, 0]))\n        rd = th.abs(self.class_rad(data[:, 1]))\n        dist = th.linalg.norm(c - d, dim=1, keepdim=True) + rc - rd\n        loss = th.relu(dist - self.margin)\n        return loss + self.class_reg(c) + self.class_reg(d)\n\n    # Loss function for normal form :math:`C \\sqcap D \\sqsubseteq E`\n    def gci1_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n        e = self.class_embed(data[:, 2])\n        rc = th.abs(self.class_rad(data[:, 0]))\n        rd = th.abs(self.class_rad(data[:, 1]))\n\n        sr = rc + rd\n        dst = th.linalg.norm(d - c, dim=1, keepdim=True)\n        dst2 = th.linalg.norm(e - c, dim=1, keepdim=True)\n        dst3 = th.linalg.norm(e - d, dim=1, keepdim=True)\n        loss = th.relu(dst - sr - self.margin) + th.relu(dst2 - rc - self.margin)\n        loss += th.relu(dst3 - rd - self.margin)\n\n        return loss + self.class_reg(c) + self.class_reg(d) + self.class_reg(e)\n\n    # Loss function for normal form :math:`C \\sqcap D \\sqsubseteq \\bot`\n    def gci1_bot_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n        rc = self.class_rad(data[:, 0])\n        rd = self.class_rad(data[:, 1])\n\n        sr = rc + rd\n        dst = th.reshape(th.linalg.norm(d - c, axis=1), [-1, 1])\n        return th.relu(sr - dst + self.margin) + self.class_reg(c) + self.class_reg(d)\n\n    # Loss function for normal form :math:`C \\sqsubseteq \\exists R. D`\n    def gci2_loss(self, data, neg=False):\n\n        if neg:\n            return self.gci2_loss_neg(data)\n\n        else:\n            # C subSelf.ClassOf R some D\n            c = self.class_embed(data[:, 0])\n            rE = self.rel_embed(data[:, 1])\n            d = self.class_embed(data[:, 2])\n\n            rc = th.abs(self.class_rad(data[:, 0]))\n            rd = th.abs(self.class_rad(data[:, 2]))\n\n            dst = th.linalg.norm(c + rE - d, dim=1, keepdim=True)\n            loss = th.relu(dst + rc - rd - self.margin)\n            return loss + self.class_reg(c) + self.class_reg(d)\n\n    # Loss function for normal form :math:`C \\nsqsubseteq \\exists R. D`\n    def gci2_loss_neg(self, data):\n\n        c = self.class_embed(data[:, 0])\n        rE = self.rel_embed(data[:, 1])\n\n        d = self.class_embed(data[:, 2])\n        rc = th.abs(self.class_rad(data[:, 1]))\n        rd = th.abs(self.class_rad(data[:, 2]))\n\n        dst = th.linalg.norm(c + rE - d, dim=1, keepdim=True)\n        loss = th.relu(rc + rd - dst + self.margin)\n        return loss + self.class_reg(c) + self.class_reg(d)\n\n    # Loss function for normal form :math:`\\exists R. C \\sqsubseteq D`\n    def gci3_loss(self, data, neg=False):\n\n        rE = self.rel_embed(data[:, 0])\n        c = self.class_embed(data[:, 1])\n        d = self.class_embed(data[:, 2])\n        rc = th.abs(self.class_rad(data[:, 1]))\n        rd = th.abs(self.class_rad(data[:, 2]))\n\n        euc = th.linalg.norm(c - rE - d, dim=1, keepdim=True)\n        loss = th.relu(euc - rc - rd - self.margin)\n        return loss + self.class_reg(c) + self.class_reg(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's first write the code containing the tranining and validation parts.\nFor that, let's use the\n:class:`EmbeddingELModel <mowl.base_models.elmodel.EmbeddingELModel>` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ELEmbeddings(EmbeddingELModel):\n\n    def __init__(self,\n                 dataset,\n                 embed_dim=50,\n                 margin=0,\n                 reg_norm=1,\n                 learning_rate=0.001,\n                 epochs=1000,\n                 batch_size=4096 * 8,\n                 model_filepath=None,\n                 device='cpu'\n                 ):\n        super().__init__(dataset, batch_size, extended=True, model_filepath=model_filepath)\n\n        self.embed_dim = embed_dim\n        self.margin = margin\n        self.reg_norm = reg_norm\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.device = device\n        self._loaded = False\n        self._loaded_eval = False\n        self.extended = False\n        self.init_model()\n            \n    def init_model(self):\n        self.model = ELEmModule(\n            len(self.class_index_dict),  # number of ontology classes\n            len(self.object_property_index_dict),  # number of ontology object properties\n            embed_dim=self.embed_dim,\n            margin=self.margin\n        ).to(self.device)\n\n    def train(self):\n        optimizer = th.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        best_loss = float('inf')\n\n        for epoch in trange(self.epochs):\n            self.model.train()\n\n            train_loss = 0\n            loss = 0\n\n            # Notice how we use the ``training_datasets`` variable directly\n            # and every element of it is a pair (GCI name, GCI tensor data).\n            for gci_name, gci_dataset in self.training_datasets.items():\n                if len(gci_dataset) == 0:\n                    continue\n\n                loss += th.mean(self.model(gci_dataset[:], gci_name))\n                if gci_name == \"gci2\":\n                    prots = [self.class_index_dict[p] for p in self.dataset.evaluation_classes.as_str]\n                    idxs_for_negs = np.random.choice(prots, size=len(gci_dataset), replace=True)\n                    rand_index = th.tensor(idxs_for_negs).to(self.device)\n                    data = gci_dataset[:]\n                    neg_data = th.cat([data[:, :2], rand_index.unsqueeze(1)], dim=1)\n                    loss += th.mean(self.model(neg_data, gci_name, neg=True))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.detach().item()\n\n            loss = 0\n            with th.no_grad():\n                self.model.eval()\n                valid_loss = 0\n                gci2_data = self.validation_datasets[\"gci2\"][:]\n                loss = th.mean(self.model(gci2_data, \"gci2\"))\n                valid_loss += loss.detach().item()\n\n            checkpoint = 1\n            if best_loss > valid_loss:\n                best_loss = valid_loss\n                th.save(self.model.state_dict(), self.model_filepath)\n            if (epoch + 1) % checkpoint == 0:\n                print(f'\\nEpoch {epoch}: Train loss: {train_loss:4f} Valid loss: {valid_loss:.4f}')\n\n    def evaluate_ppi(self):\n        self.init_model()\n        print('Load the best model', self.model_filepath)\n        self.model.load_state_dict(th.load(self.model_filepath))\n        with th.no_grad():\n            self.model.eval()\n\n            eval_method = self.model.gci2_loss\n\n            evaluator = ELEmbeddingsPPIEvaluator(\n                self.dataset.testing, eval_method, self.dataset.ontology,\n                self.class_index_dict, self.object_property_index_dict, device=self.device)\n            evaluator()\n            evaluator.print_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mowl.datasets.builtin import PPIYeastSlimDataset\n\ndataset = PPIYeastSlimDataset()\n\nmodel = ELEmbeddings(dataset,\n                     embed_dim=10,\n                     margin=0.1,\n                     reg_norm=1,\n                     learning_rate=0.001,\n                     epochs=20,\n                     batch_size=4096,\n                     model_filepath=None,\n                     device='cpu')\n\nmodel.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
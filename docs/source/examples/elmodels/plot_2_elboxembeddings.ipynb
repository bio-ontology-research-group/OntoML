{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ELBoxEmbeddings\n\nThis example is based on the paper [Description Logic EL++ Embeddings with Intersectional Closure](https://arxiv.org/abs/2202.14018v1). This paper is based on the idea of :doc:`/examples/elmodels/plot_1_elembeddings`, but in this work the main point is to solve the *intersectional closure* problem.\n\nIn the case of :doc:`/examples/elmodels/plot_1_elembeddings`, the geometric objects representing ontology classes are $n$-dimensional balls. One of the normal forms in EL is:\n\n\\begin{align}C_1 \\sqcap C_2 \\sqsubseteq D\\end{align}\n\nAs we can see, there is an intersection operation $C_1 \\sqcap C_2$. Computing this intersection using balls is not a closed operations because the region contained in the intersection of two balls is not a ball. To solve that issue, this paper proposes the idea of changing the geometric objects to boxes, for which the intersection operation has the closure property.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This example is quite similar to the one found in :doc:`/examples/elmodels/plot_1_elembeddings`.\nThere might be slight changes in the training part but the most important changes are in the\n`Definition of loss functions`_ definition of the loss functions for each normal form.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mowl\nmowl.init_jvm(\"10g\")\n\nfrom mowl.base_models.elmodel import EmbeddingELModel\nimport mowl.models.elboxembeddings.losses as L\nfrom mowl.nn.elmodule import ELModule\nimport math\nimport logging\nimport numpy as np\n\nfrom mowl.models.elboxembeddings.evaluate import ELBoxEmbeddingsPPIEvaluator\nfrom mowl.projection import TaxonomyWithRelationsProjector\n    \nfrom tqdm import trange, tqdm\n\nimport torch as th\nfrom torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of loss functions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ELBoxModule(ELModule):\n\n    def __init__(self, nb_ont_classes, nb_rels, embed_dim=50, margin=0.1):\n        super().__init__()\n        self.nb_ont_classes = nb_ont_classes\n        self.nb_rels = nb_rels\n\n        self.embed_dim = embed_dim\n\n        self.class_embed = nn.Embedding(self.nb_ont_classes, embed_dim)\n        nn.init.uniform_(self.class_embed.weight, a=-1, b=1)\n\n        weight_data = th.linalg.norm(self.class_embed.weight.data, axis=1).reshape(-1, 1)\n        self.class_embed.weight.data /= weight_data\n\n        self.class_offset = nn.Embedding(self.nb_ont_classes, embed_dim)\n        nn.init.uniform_(self.class_offset.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.class_offset.weight.data, axis=1).reshape(-1, 1)\n        self.class_offset.weight.data /= weight_data\n\n        self.rel_embed = nn.Embedding(nb_rels, embed_dim)\n        nn.init.uniform_(self.rel_embed.weight, a=-1, b=1)\n        weight_data = th.linalg.norm(self.rel_embed.weight.data, axis=1).reshape(-1, 1)\n        self.rel_embed.weight.data /= weight_data\n\n        self.margin = margin\n\n    def gci0_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n\n        euc = th.abs(c - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc + off_c - off_d + self.margin), axis=1),\n                         [-1, 1])\n\n        return dst\n\n    def gci1_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n        e = self.class_embed(data[:, 2])\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n        off_e = th.abs(self.class_offset(data[:, 2]))\n\n        startAll = th.maximum(c - off_c, d - off_d)\n        endAll = th.minimum(c + off_c, d + off_d)\n\n        new_offset = th.abs(startAll - endAll) / 2\n\n        cen1 = (startAll + endAll) / 2\n        euc = th.abs(cen1 - e)\n\n        dst = th.reshape(th.linalg.norm(th.relu(euc + new_offset - off_e + self.margin), axis=1),\n                         [-1, 1]) + th.linalg.norm(th.relu(startAll - endAll), axis=1)\n        return dst\n\n    def gci1_bot_loss(self, data, neg=False):\n        c = self.class_embed(data[:, 0])\n        d = self.class_embed(data[:, 1])\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(data[:, 1]))\n\n        euc = th.abs(c - d)\n        dst = th.reshape(th.linalg.norm(th.relu(-euc + off_c + off_d + self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n    def gci2_loss(self, data, neg=False):\n        if neg:\n            return self.gci2_loss_neg(data)\n        else:\n            c = self.class_embed(data[:, 0])\n            r = self.rel_embed(data[:, 1])\n            d = self.class_embed(data[:, 2])\n\n            off_c = th.abs(self.class_offset(data[:, 0]))\n            off_d = th.abs(self.class_offset(data[:, 2]))\n\n            euc = th.abs(c + r - d)\n            dst = th.reshape(th.linalg.norm(th.relu(euc + off_c - off_d + self.margin), axis=1),\n                             [-1, 1])\n            return dst\n\n    def gci2_loss_neg(self, data):\n        c = self.class_embed(data[:, 0])\n        r = self.rel_embed(data[:, 1])\n\n        rand_index = np.random.choice(self.class_embed.weight.shape[0], size=len(data))\n        rand_index = th.tensor(rand_index).to(self.class_embed.weight.device)\n        d = self.class_embed(rand_index)\n\n        off_c = th.abs(self.class_offset(data[:, 0]))\n        off_d = th.abs(self.class_offset(rand_index))\n\n        euc = th.abs(c + r - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc - off_c - off_d - self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n    def gci3_loss(self, data, neg=False):\n        r = self.rel_embed(data[:, 0])\n        c = self.class_embed(data[:, 1])\n        d = self.class_embed(data[:, 2])\n\n        off_c = th.abs(self.class_offset(data[:, 1]))\n        off_d = th.abs(self.class_offset(data[:, 2]))\n\n        euc = th.abs(c - r - d)\n        dst = th.reshape(th.linalg.norm(th.relu(euc - off_c - off_d + self.margin), axis=1),\n                         [-1, 1])\n        return dst\n\n\nclass ELBoxEmbeddings(EmbeddingELModel):\n\n    def __init__(self,\n                 dataset,\n                 embed_dim=50,\n                 margin=0,\n                 reg_norm=1,\n                 learning_rate=0.001,\n                 epochs=1000,\n                 batch_size=4096 * 8,\n                 model_filepath=None,\n                 device='cpu'\n                 ):\n        super().__init__(dataset, batch_size, extended=True, model_filepath=model_filepath)\n\n        self.embed_dim = embed_dim\n        self.margin = margin\n        self.reg_norm = reg_norm\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.device = device\n        self._loaded = False\n        self._loaded_eval = False\n        self.extended = False\n        self.init_model()\n        self._testing_set = None\n        self._training_set = None\n        self._head_entities = None\n\n    @property\n    def testing_set(self):\n        if self._testing_set is None:\n            projector = TaxonomyWithRelationsProjector(taxonomy=False,\n                                                       relations=[\"http://interacts_with\"]\n                                                       )\n            self._testing_set = projector.project(dataset.testing)\n        return self._testing_set\n\n    @property\n    def training_set(self):\n        if self._training_set is None:\n            projector = TaxonomyWithRelationsProjector(taxonomy=False,\n                                                       relations=[\"http://interacts_with\"]\n                                                       )\n            self._training_set = projector.project(dataset.ontology)\n        return self._training_set\n\n    @property\n    def head_entities(self):\n        if self._head_entities is None:\n            self._head_entities = self.dataset.evaluation_classes.as_str\n        return self._head_entities\n\n    @property\n    def tail_entities(self):\n        return self.head_entities\n    \n    def init_model(self):\n        self.model = ELBoxModule(\n            len(self.class_index_dict),\n            len(self.object_property_index_dict),\n            embed_dim=self.embed_dim,\n            margin=self.margin\n        ).to(self.device)\n\n    def load_best_model(self):\n        self.model.load_state_dict(th.load(self.model_filepath))\n        \n\n    def train(self):\n        criterion = nn.MSELoss()\n        optimizer = th.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        best_loss = float('inf')\n\n        training_datasets = {k: v.data for k, v in\n                             self.training_datasets.items()}\n        validation_dataset = self.validation_datasets[\"gci2\"][:]\n\n        for epoch in trange(self.epochs):\n            self.model.train()\n\n            train_loss = 0\n            loss = 0\n            for gci_name, gci_dataset in training_datasets.items():\n                if len(gci_dataset) == 0:\n                    continue\n                rand_index = np.random.choice(len(gci_dataset), size=512)\n                dst = self.model(gci_dataset[rand_index], gci_name)\n                mse_loss = criterion(dst, th.zeros(dst.shape, requires_grad=False).to(self.device))\n                loss += mse_loss\n\n                if gci_name == \"gci2\":\n                    rand_index = np.random.choice(len(gci_dataset), size=512)\n                    gci_batch = gci_dataset[rand_index]\n                    prots = [self.class_index_dict[p] for p in self.dataset.evaluation_classes.as_str]\n                    idxs_for_negs = np.random.choice(prots, size=len(gci_batch), replace=True)\n                    rand_prot_ids = th.tensor(idxs_for_negs).to(self.device)\n                    neg_data = th.cat([gci_batch[:, :2], rand_prot_ids.unsqueeze(1)], dim=1)\n\n                    dst = self.model(neg_data, gci_name, neg=True)\n                    mse_loss = criterion(dst,\n                                         th.ones(dst.shape, requires_grad=False).to(self.device))\n                    loss += mse_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.detach().item()\n\n            with th.no_grad():\n                self.model.eval()\n                valid_loss = 0\n                gci2_data = validation_dataset\n                dst = self.model(gci2_data, \"gci2\")\n                loss = criterion(dst, th.zeros(dst.shape, requires_grad=False).to(self.device))\n                valid_loss += loss.detach().item()\n\n            checkpoint = 500\n            if best_loss > valid_loss:\n                best_loss = valid_loss\n                th.save(self.model.state_dict(), self.model_filepath)\n            if (epoch + 1) % checkpoint == 0:\n                print(f'\\nEpoch {epoch+1}: Train loss: {train_loss:.4f} Valid loss: {valid_loss:.4f}')\n\n    def evaluate_ppi(self):\n        self.init_model()\n        print('Load the best model', self.model_filepath)\n        self.model.load_state_dict(th.load(self.model_filepath))\n        with th.no_grad():\n            self.model.eval()\n\n            eval_method = self.model.gci2_loss\n\n            evaluator = ELBoxEmbeddingsPPIEvaluator(\n                self.dataset.testing, eval_method, self.dataset.ontology,\n                self.class_index_dict, self.object_property_index_dict, device=self.device)\n            evaluator()\n            evaluator.print_metrics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mowl.datasets.builtin import PPIYeastSlimDataset\n\ndataset = PPIYeastSlimDataset()\n\nmodel = ELBoxEmbeddings(dataset,\n                     embed_dim=50,\n                     margin=-0.05,\n                     reg_norm=1,\n                     learning_rate=0.001,\n                     epochs=10000,\n                     batch_size=4096,\n                     model_filepath=None,\n                     device='cpu')\n\nmodel.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating the model\n\nNow, it is time to evaluate embeddings. For this, we use the\n:class:`ModelRankBasedEvaluator <mowl.evaluation.ModelRankBasedEvaluator>` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mowl.evaluation.rank_based import ModelRankBasedEvaluator\n\nwith th.no_grad():                                                                        \n    model.load_best_model()                                                               \n    evaluator = ModelRankBasedEvaluator(                                                  \n        model,                                                                            \n        device = \"cpu\",\n        eval_method = model.model.gci2_loss                                               \n    )                                                                                         \n                                                                                                  \n    evaluator.evaluate(show=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}